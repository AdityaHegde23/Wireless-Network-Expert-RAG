# -*- coding: utf-8 -*-
"""seminar_final2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C7nRpAcd55lp3W49XYFgGY5shWs707o5
"""

import os
import dspy
from PyPDF2 import PdfReader

# Directory where wireless networking standards are stored
data_directory = "wireless_specs/"

# Create the directory if it doesn't exist (for testing purposes)
if not os.path.exists(data_directory):
    os.makedirs(data_directory)

# Create a sample text file for testing (optional)
sample_file_path = os.path.join(data_directory, "sample_spec.txt")
with open(sample_file_path, "w") as f:
    f.write("This is a sample wireless networking specification for testing purposes.")

parsed_data = []

def parse_specifications(data_directory):
    for file_name in os.listdir(data_directory):
        file_path = os.path.join(data_directory, file_name)

        if file_name.endswith(".txt"):
            with open(file_path, "r") as file:
                parsed_content = file.read()
                parsed_data.append(parsed_content)

        elif file_name.endswith(".pdf"):
            with open(file_path, "rb") as file:
                reader = PdfReader(file)
                pdf_content = ""
                for page in reader.pages:
                    pdf_content += page.extract_text()
                parsed_data.append(pdf_content)

    return parsed_data

# Execute parsing
parsed_data = parse_specifications(data_directory)
print(f"Parsed {len(parsed_data)} documents.")

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer

# Loading the OPT model and tokenizer
model_name = "facebook/opt-125m"  # Using a smaller OPT model to reduce GPU memory usage
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

import torch
from torch.utils.data import Dataset
from transformers import DataCollatorWithPadding

class WirelessSpecsDataset(Dataset):
    def __init__(self, texts, tokenizer):
        self.texts = texts
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        encodings = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=256,  # Reduced sequence length to reduce memory usage
            return_tensors='pt'
        )

        input_ids = encodings["input_ids"].squeeze()
        attention_mask = encodings["attention_mask"].squeeze()

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": input_ids
        }

# Create dataset
dataset = WirelessSpecsDataset(parsed_data, tokenizer)

# Create a data collator to handle padding dynamically for each batch
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

import os
os.environ["WANDB_MODE"] = "disabled"  # Disable Weights & Biases

training_args = TrainingArguments(
    output_dir="./output",
    num_train_epochs=3,
    per_device_train_batch_size=1,  # Small batch size to fit GPU memory
    gradient_accumulation_steps=4,  # Simulate a larger batch size
    save_steps=10,
    save_total_limit=2,
    warmup_steps=10,
    weight_decay=0.01,
    logging_dir="./logs",
    fp16=True  # Enable mixed precision for better memory efficiency
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator
)

trainer.train()

# Save the model and tokenizer after training
model.save_pretrained('./fine_tuned_model')
tokenizer.save_pretrained('./fine_tuned_model')

from transformers import pipeline

# Load the fine-tuned model from the saved directory
generator = pipeline('text-generation', model='./fine_tuned_model', tokenizer='./fine_tuned_model')

# Test the model with a more structured prompt and enable sampling
prompt = "Explain the purpose of MAC in wireless networking, focusing on data transmission and security."
response = generator(
    prompt,
    max_length=100,
    num_return_sequences=1,
    do_sample=True,  # Enable sampling for more diverse outputs
    temperature=0.7,  # Controls randomness of the output
    top_p=0.9  # Nucleus sampling for controlling the diversity
)

print(f"Question: {prompt}\nAnswer: {response[0]['generated_text']}")

from transformers import pipeline

# Load a model fine-tuned for question answering
qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

# Provide a context and a question
context = """
The Media Access Control (MAC) layer in wireless networking is responsible for controlling how data packets are placed onto and taken off of the transmission medium.
It ensures that devices can communicate without interference by using protocols that manage access to the wireless medium.
The MAC layer also includes security mechanisms to protect data transmissions, such as encryption and authentication processes, which ensure that only authorized devices can access the network and transmit data securely.
"""
question = "Explain the responsibilities of the MAC layer in wireless networking."

# Generate an answer
response = qa_pipeline(question=question, context=context)
print(f"Question: {question}\nAnswer: {response['answer']}")

from transformers import pipeline

# Load the fine-tuned model from the saved directory
generator = pipeline('text-generation', model='./fine_tuned_model', tokenizer='./fine_tuned_model')

# Test the model with a more structured prompt and enable sampling
prompt = "Describe the evolution of wireless networking protocols "
response = generator(
    prompt,
    max_length=100,
    num_return_sequences=1,
    do_sample=True,  # Enable sampling for more diverse outputs
    temperature=0.7,  # Controls randomness of the output
    top_p=0.9  # Nucleus sampling for controlling the diversity
)

print(f"Question: {prompt}\nAnswer: {response[0]['generated_text']}")
